{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Copy of classification-notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH5EKJhqr1gz"
      },
      "source": [
        "# Image Classification for Coffee or Donuts\n",
        "\n",
        "Is the given image a coffee photo, or a donut?\n",
        "In this demo notebook we build a simple deep neural network to classify an image as one of the following:\n",
        "- coffee\n",
        "- mug\n",
        "- donut\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79UE8NLVvl7g"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHQyc83kr1g7"
      },
      "source": [
        "credentials = {\n",
        "  \"bucket\": \"xdtest\",\n",
        "  \"access_key_id\": \"da2ab693a5934be19ac1709d6c1c6367\",\n",
        "  \"secret_access_key\": \"06daaa57b3fa8082af7620dd402c647e1a14e9ab9a4a7714\",\n",
        "  \"endpoint_url\": \"https://s3.us.cloud-object-storage.appdomain.cloud\"\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNDUCpkfr1g0"
      },
      "source": [
        "## Install Prerequisites\n",
        "\n",
        "We use Keras/Tensorflow to build the classification model, and visualize the process with matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xjPdHZor1g1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c5e4cff-dca7-42d0-dbc8-bbca7ac9653d"
      },
      "source": [
        "!pip install tensorflow==1.15 ibm-cos-sdk==2.6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 29kB/s \n",
            "\u001b[?25hCollecting ibm-cos-sdk==2.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/91/86b2816c7b77d816b03a1ad6cf7db4b1f67556af395d5b93fdae6086c933/ibm-cos-sdk-2.6.0.tar.gz (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 28.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 44.2MB/s \n",
            "\u001b[?25hCollecting ibm-cos-sdk-core==2.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/c1/c823507c472bf88dbd045445df6850744111d34fd218c6ea3b9c9bde2cfe/ibm-cos-sdk-core-2.6.0.tar.gz (763kB)\n",
            "\u001b[K     |████████████████████████████████| 768kB 48.7MB/s \n",
            "\u001b[?25hCollecting ibm-cos-sdk-s3transfer==2.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/92/682a28b99777a3fdc65e6d5641ed7e1ca470d0eab3bb2826cc30c6b60e21/ibm-cos-sdk-s3transfer-2.6.0.tar.gz (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 48.7MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (56.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n",
            "Collecting docutils<0.16,>=0.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 43.6MB/s \n",
            "\u001b[?25hCollecting requests<2.23,>=2.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from ibm-cos-sdk-core==2.6.0->ibm-cos-sdk==2.6) (2.8.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.0.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<2.23,>=2.18->ibm-cos-sdk-core==2.6.0->ibm-cos-sdk==2.6) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<2.23,>=2.18->ibm-cos-sdk-core==2.6.0->ibm-cos-sdk==2.6) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<2.23,>=2.18->ibm-cos-sdk-core==2.6.0->ibm-cos-sdk==2.6) (1.24.3)\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Building wheels for collected packages: ibm-cos-sdk, gast, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer\n",
            "  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk: filename=ibm_cos_sdk-2.6.0-py2.py3-none-any.whl size=72545 sha256=591b14b5a469a397fe92995315c0b0b0e54da6bc07a7c32156f48ed9b604b6d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/9c/c4/a2c610ccb877d37c2cb87a5bfe55845fecffd6bb01bcd5e9d5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6H0k7V3r1g4"
      },
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import uuid\n",
        "import shutil\n",
        "import json\n",
        "from botocore.client import Config\n",
        "import ibm_boto3\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XnKLRuRr1g6"
      },
      "source": [
        "## Read The Data\n",
        "\n",
        "Here we build simple wrapper functions to read in the data from our cloud object storage buckets and extract it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCV4Wy2rr1g-"
      },
      "source": [
        "A download function from IBM Cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf1Ms-Lgr1g-"
      },
      "source": [
        "def download_file_cos(credentials, local_file_name, key): \n",
        "    '''\n",
        "    Wrapper function to download a file from cloud object storage using the\n",
        "    credential dict provided and loading it into memory\n",
        "    '''\n",
        "    cos = ibm_boto3.client(\n",
        "        service_name='s3',\n",
        "        aws_access_key_id=credentials['access_key_id'],\n",
        "        aws_secret_access_key=credentials['secret_access_key'],\n",
        "        endpoint_url=credentials['endpoint_url'])\n",
        "    try:\n",
        "        res=cos.download_file(Bucket=credentials['bucket'], Key=key, Filename=local_file_name)\n",
        "    except Exception as e:\n",
        "        print(Exception, e)\n",
        "    else:\n",
        "        print('File Downloaded')\n",
        "\n",
        "def get_annotations(credentials): \n",
        "    cos = ibm_boto3.client(\n",
        "        service_name='s3',\n",
        "        aws_access_key_id=credentials['access_key_id'],\n",
        "        aws_secret_access_key=credentials['secret_access_key'],\n",
        "        endpoint_url=credentials['endpoint_url'])\n",
        "    try:\n",
        "        return json.loads(cos.get_object(Bucket=credentials['bucket'], Key='_annotations.json')['Body'].read())\n",
        "    except Exception as e:\n",
        "        print(Exception, e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfBjP61Mr1hB"
      },
      "source": [
        "base_path = 'data'\n",
        "if os.path.exists(base_path) and os.path.isdir(base_path):\n",
        "    shutil.rmtree(base_path)\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "annotations = get_annotations(credentials)\n",
        "\n",
        "for i, image in enumerate(annotations['annotations'].keys()):\n",
        "    label = annotations['annotations'][image][0]['label']\n",
        "    os.makedirs(os.path.join(base_path, label), exist_ok=True)\n",
        "    _, extension = os.path.splitext(image)\n",
        "    local_path = os.path.join(base_path, label, str(uuid.uuid4()) + extension)\n",
        "    download_file_cos(credentials, local_path, image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "M1HMNpZYr1hD"
      },
      "source": [
        "!ls data/coffee\n",
        "!ls data/donut\n",
        "!ls data/mug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qpYZMNer1hG"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "We start with a [MobileNetV2](https://arxiv.org/abs/1801.04381) architecture as the backbone [pretrained feature extractor](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet). We then add a couple of dense layers and a softmax layer to perfom the classification. We freeze the MobileNetV2 backbone with weights trained on ImageNet dataset and only train the dense layers and softmax layer that we have added."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN_FY40Dr1hG"
      },
      "source": [
        "base_model=tf.keras.applications.MobileNetV2(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
        "x=base_model.output\n",
        "x=tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x=tf.keras.layers.Dense(512,activation='relu')(x) #dense layer 1\n",
        "x=tf.keras.layers.Dense(256,activation='relu')(x) #dense layer 2\n",
        "preds=tf.keras.layers.Dense(3,activation='softmax')(x) #final layer with softmax activation\n",
        "\n",
        "model=tf.keras.Model(inputs=base_model.input,outputs=preds)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApSMk3W2r1hJ"
      },
      "source": [
        "#Freeze layers from MobileNetV2 backbone (not to be trained)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHEvYJARr1hM"
      },
      "source": [
        "#Prepare the training dataset as a data generator object\n",
        "train_datagen=tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input) #included in our dependencies\n",
        "\n",
        "train_generator=train_datagen.flow_from_directory('data',\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=10,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv_R_SCFr1hO"
      },
      "source": [
        "### Using Adam, categorical_crossentropy and accuracy as optimization method, loss function and metrics, respectively"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93AFFFdtr1hO"
      },
      "source": [
        "# Build the model\n",
        "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjj9cjU3r1hR"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "exRobg6cr1hS"
      },
      "source": [
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)\n",
        "step_size_train=5\n",
        "log_file = model.fit_generator(generator=train_generator,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                   epochs=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNRpVg-zr1hU"
      },
      "source": [
        "## Figure of Training Loss and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C6J2Gubr1hV"
      },
      "source": [
        "# Model accuracy and loss vs epoch\n",
        "plt.plot(log_file.history['acc'], '-bo', label=\"train_accuracy\")\n",
        "plt.plot(log_file.history['loss'], '-r*', label=\"train_loss\")\n",
        "plt.title('Training Loss and Accuracy')\n",
        "plt.ylabel('Loss/Accuracy')\n",
        "plt.xlabel('Epoch #')\n",
        "plt.legend(loc='center right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3pu-2Ddr1hX"
      },
      "source": [
        "## Model Performance\n",
        "\n",
        "Here we perform inference on some sample data points to determine the performance of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD_GuzqAr1hX"
      },
      "source": [
        "# Mapping labels \n",
        "label_map = (train_generator.class_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl-eHspBr1hZ"
      },
      "source": [
        "label_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQENLlakr1hc"
      },
      "source": [
        "# Creating a sample inference function\n",
        "def prediction(image_path, model):\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
        "    x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = tf.keras.applications.mobilenet_v2.preprocess_input(x)\n",
        "    preds = model.predict(x)\n",
        "    #print('Predictions', preds)\n",
        "    \n",
        "    for pred, value in label_map.items():    \n",
        "        if value == np.argmax(preds):\n",
        "            print('Predicted class is:', pred)\n",
        "            print('With a confidence score of: ', np.max(preds))\n",
        "    \n",
        "    return np.argmax(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jJ3EsC0r1he"
      },
      "source": [
        "coffee_url = 'https://i5.walmartimages.com/asr/559143bb-3fd0-41b0-8358-fb0f74c41f8d_1.61dbeaff765619bbba67d4e519174932.jpeg'\n",
        "mug_url = 'https://cdn.cnn.com/cnnnext/dam/assets/150929101049-black-coffee-stock-super-tease.jpg'\n",
        "donut_url = 'https://i.ytimg.com/vi/gevpzxRxec4/maxresdefault.jpg'\n",
        "!wget {coffee_url} -O Coffee.jpg \n",
        "!wget {mug_url} -O Mug.jpg\n",
        "!wget {donut_url} -O Donut.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5o2uwXLr1hi"
      },
      "source": [
        "#Opening first image\n",
        "image = Image.open(\"Coffee.jpg\")\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKrocwF2r1hk"
      },
      "source": [
        "#performing inference on above image\n",
        "prediction('Coffee.jpg', model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6PJOcayr1hm"
      },
      "source": [
        "#Opening second image\n",
        "image = Image.open(\"Mug.jpg\")\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxsWOQ3Ir1hp"
      },
      "source": [
        "prediction('Mug.jpg', model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lhwp01Fr1hs"
      },
      "source": [
        "#Opening third image\n",
        "image = Image.open(\"Donut.jpg\")\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SH5afqlr1ht"
      },
      "source": [
        "prediction('Donut.jpg', model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}